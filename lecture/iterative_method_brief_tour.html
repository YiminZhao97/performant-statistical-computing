<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Brief tour of iterative methods</title>
    <meta charset="utf-8" />
    <meta name="author" content="Aki Nishimura" />
    <meta name="date" content="2021-04-22" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="extra.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Brief tour of iterative methods
## Under-utilized tools of numerical linear algebra
### Aki Nishimura
### 2021-04-22

---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    extensions: ["color.js"],
    Macros: {
      bx: "{\\boldsymbol{x}}",
      by: "{\\boldsymbol{y}}",
      bf: "{\\boldsymbol{f}}",
      bA: "{\\boldsymbol{A}}",
      bdelta: "{\\boldsymbol{\\delta}}",
      bSigma: "{\\boldsymbol{\\Sigma}}",
      given: "{\\, | \\,}",
      eqDistribution: "{\\overset{\\small \\textrm{d}}{=}}",
      iid: "{\\mathbin{\\overset{\\small \\textrm{i.i.d.}}{\\sim}}}",
      normalDist: "{\\mathcal{N}}"
    }
  }
});
</script>

<span style="display: none;">
  $$\newcommand{\bm}[1]{\boldsymbol{#1}}$$
  $$\definecolor{lava}{rgb}{0.81, 0.06, 0.13}$$
</span>



# What are iterative methods?

Direct v.s. iterative solutions:

&lt;img src="figure/iterative_vs_direct_method_visualization.png" alt="iterative vs direct method" class="center" style="width: 90%;"/&gt;

---

# Iterative methods: broad definition

Examples of direct methods: 
* Solving a quadratic, or quartic, equation
* Least-squre via QR decomposition

--

Examples of iterative methods (under broad definition):
* Minimizing a function via Newton's method
* Full eigenvalue decomposition `\(\bm{A} = \bm{U} \bm{\Lambda} \bm{U}^\intercal\)`

---

# Iterative methods: narrow definition

In numerical linalg, "iterative methods" often refer to algorithms that rely solely on the operation `\(\bm{v} \to \bm{A} \bm{v}\)`.

--

&lt;p style="margin-top:2.5ex;"&gt; &lt;/p&gt;
For example, many iterative methods for
* solving `\(\bm{A} \bx = \bm{b}\)`
* finding largest and smallest eigenvalues/vectors of `\(\bm{A}\)`
* minimizing `\(\| \bm{A} \bx - \bm{b} \|^2\)` (or `\(\| \bm{X} \bm{\beta} - \bm{y} \|^2\)`)

&lt;p style="margin-top:-1ex;"&gt; &lt;/p&gt;
only require operations of the form `\(\bm{v} \to \bm{A} \bm{v}\)`. 

--

&lt;p style="margin-top:2.5ex;"&gt; &lt;/p&gt;
These algorithms are _agnostic_ to how computer represents `\(\bm{A}\)` and calculates `\(\bm{v} \to \bm{A} \bm{v}\)`.

---

# What's the big deal w/ iterative methods?

## Wrong-but-useful argument in terms algorithmic complexity:
* Most direct methods have `\(O(\min\{n p^2, n^2 p\})\)` costs.
* Iterative methods require `\(k\)` matrix-vector multi-plication `\(\bm{v} \to \bm{A} \bm{v}\)` where `\(k \ll \min\{n, p\}\)` potentially.

---

# What's the big deal w/ iterative methods?

## Better arguments
In many modern applications, the matrix of interest `\(\bm{A}\)`
* has special structure for fast `\(\bm{v} \to \bm{A} \bm{v}\)` operations.
* is so big that storing it explicitly &amp;mdash; as required for direct methods &amp;mdash; is impossible.

--

For example, `\(\bm{A}\)` may corresond to
* design matrix for sparse binary features
* sparse precision/inverse-covariance matrix
--

* matrix with underlying graph/tree structure
* wavelet transform for decomposing singnals

---
layout: true

# When you might want iterative methods

## Example: centered (and standardized) sparse binary design

---


```r
n_obs &lt;- 10^4
n_pred &lt;- 10^3
X &lt;- simulate_sparse_binary_design(
  n_obs, n_pred, density = .01, seed = 140850
)
```

--

Sparse matrix `\(\bm{X}\)` won't be sparse after `\(\bm{X} \gets \bm{X} - \bm{1} \cdot \bm{\bar{x}}^\intercal\)`.

Nor binary matrix `\(\bm{X}\)` binary after `\(\bm{X} \gets \bm{X} \cdot \textrm{diag}(\bm{\sigma}_{\bm{X}}^{-1})\)`.

--

Bye-bye memory efficiency: 

.center[1% dense -&gt; 100% dense &amp;nbsp;&amp;nbsp; &amp; &amp;nbsp;&amp;nbsp; 1 bit -&gt; 64 bits]

Beg for a fancy computer, or go home and cry.

---

But wait! You can multiply by a centered and standardized matrix _without_ ever centering or standarding explicitly:

`$$\textrm{diag}(\bm{\sigma}_{\bm{X}}^{-1}) \cdot \bm{v} = \bm{v} / \bm{\sigma}_{\bm{X}}$$`
`$$\left(\bm{X} - \bm{1} \cdot \bm{\bar{x}}^\intercal \right) \bm{v} = \bm{X} \bm{v} - (\bm{\bar{x}}^\intercal \bm{v}) \bm{1}$$`

---




```r
pred_mean &lt;- colMeans(X)
pred_var &lt;- pred_mean * (1 - pred_mean)
pred_sd &lt;- sqrt(pred_var)

standardized_matvec &lt;- function(X, v) {
  v &lt;- v / pred_sd
  result &lt;- X %*% v
  result &lt;- result - sum(pred_mean * v)
  return(as.vector(result))
}
```

---


```r
v &lt;- rnorm(n_pred)
efficient_matvec_output &lt;- standardized_matvec(X, v)

X_dense &lt;- as.matrix(X)
for (j in 1:n_pred) {
  X_dense[, j] &lt;- (X_dense[, j] - pred_mean[j]) / pred_sd[j]
}
brute_matvec_output &lt;- as.vector(X_dense %*% v)
```

--


```r
assert("Two outputs agree",
  are_all_close(
    efficient_matvec_output, 
    brute_matvec_output, 
    rel_tol = 1e-3
  )
)
```


---
layout: false

# When you might want iterative methods

## Example: graphical model / Gaussian Markov random fields 

**Fact:** If `\(\bx \sim \normalDist(\bm{\mu}, \bm{\Phi}^{-1})\)`, then 
`\(\Phi_{ij} = 0 \ \Leftrightarrow \ x_i \indep x_j \given \bx_{-ij}.\)`

--

&lt;img src="figure/inla_gmrf_approximatin.png" alt="Markovian approximation of Matern field" class="center" style="width: 80%;"/&gt;

---

# When you might want iterative methods

## Example: graphical model / Gaussian Markov random fields 

**Fact:** If `\(\bx \sim \normalDist(\bm{\mu}, \bm{\Phi}^{-1})\)`, then 
`\(\Phi_{ij} = 0 \ \Leftrightarrow \ x_i \indep x_j \given \bx_{-ij}.\)`

&lt;img src="figure/graphical_lasso_cell_signaling_example.jpeg" alt="Flow cytometry data of 11 proteins" class="center" style="width: 80%;"/&gt;

---

# Power iteration for largest eigenvec/val

### ( `\(\in\)` Broad-sense iterative methods)

--

Albeit unnecessary, let's assume `\(\bm{A}\)` symmetric so that
`$$\bm{A} = \bm{U} \bm{\Lambda} \bm{U}^\intercal = \sum_i \lambda_i \bm{u}_i \bm{u}_i^\intercal \ \text{ for } \ \lambda_1 \geq \lambda_2 \geq \ldots.$$`

--

This means that
`$$\bm{A}^k = \bm{U} \bm{\Lambda}^k \bm{U}^\intercal = \sum_i \lambda_i^k \bm{u}_i \bm{u}_i^\intercal.$$`

--

If `\(\lambda_1 &gt; \lambda_2\)`, eventually `\(\lambda_1^k \gg \lambda_2^k\)` and `\(\bm{A}^k \approx \lambda_1^k \bm{u}_1 \bm{u}_1\)`.

---

# Power iteration for largest eigenvec/val

You'd be nuts to compute `\(\bm{A}^k\)`.

---
layout: true

# Power iteration for largest eigenvec/val

You'd be nuts to compute `\(\bm{A}^k\)`, _but_ can iterate `\(\bm{v} \to \bm{A} \bm{v}\)`!

---


```r
power_iterate &lt;- function (A, init_vec, n_iter) {
  v &lt;- init_vec
  for (k in 1:n_iter) {
    v &lt;- A %*% v
    v &lt;- v / sqrt(sum(v^2)) # $\lambda_1^k$ might ex/inplode o.w.
  }
  ...
}
```

--

This gives us `\(\bm{v} \approx \bm{u}_1\)`. Now, how do we approximate `\(\lambda_1\)`?

--

Use _Rayleigh-Ritz method_ &amp;mdash; `\(\, \lambda_1 \approx \bm{v}^\intercal \bm{A} \bm{v} \,\)` because
`$$\bm{u}_1^\intercal \bm{A} \bm{u}_1 = \bm{u}_1^\intercal \left( \sum_i \lambda_i \bm{u}_i \bm{u}_i^\intercal \right) \bm{u}_1 = \lambda_1.$$`

---


```r
power_iterate &lt;- function (A, init_vec, n_iter) {
  v &lt;- init_vec
  for (k in 1:n_iter) {
    v &lt;- A %*% v
    v &lt;- v / sqrt(sum(v^2)) # $\lambda_1^k$ might ex/inplode o.w.
  }
  ...
}
```

More generally, we have
`$$\lambda_\max = \max_{\|\bm{v}\| = 1} \bm{v}^\intercal \bm{A} \bm{v} \ \, \text{ and } \ \,
\lambda_\min = \min_{\|\bm{v}\| = 1} \bm{v}^\intercal \bm{A} \bm{v}.$$`

---


```r
power_iterate &lt;- function (A, init_vec, n_iter) {
  v &lt;- init_vec
  for (k in 1:n_iter) {
    v &lt;- A %*% v
    v &lt;- v / sqrt(sum(v^2)) # $\lambda_1^k$ might ex/inplode o.w.
  }
  eigvec_est &lt;- v
  eigval_est &lt;- t(eigvec_est) %*% A %*% eigvec_est
  return(list(eigvec_est, eigval_est))
}
```

--

**Fun fact:** Google's PageRank uses the power iteration. &lt;br&gt; (For a non-symmetric stochastic matrix `\(\bm{A}\)`.)

---
layout: false
class: center, middle, inverse

# Krylov subspace: "the" iterative methods

---

# Krylov subspace methods 

One lesson from power iteration:&lt;br&gt; 

.center[
  `\(\bm{A}^k \bm{v}\)` tells you a lot about the largest eigenvec/val.
]

&lt;p style="margin-top:3ex;"&gt; &lt;/p&gt;

--

Here is a more striking fact:
.center[
  the collection `\(\{ \bm{v}, \bm{A} \bm{v}, \bm{A}^2 \bm{v}, \ldots, \bm{A}^k \bm{v} \}\)` tells you a lot about the extreme (largest and smallest) eigenvec/vals.
]

&lt;p style="margin-top:3ex;"&gt; &lt;/p&gt;

--

The space 
`$$\mathcal{K}_{k + 1}(\bm{A}, \bm{v}) = \textrm{span}\{\bm{v}, \bm{A} \bm{v}, \bm{A}^2 \bm{v}, \ldots, \bm{A}^k \bm{v} \}$$`
is called the _Krylov subspace_.

---

# Taste of Krylov methods: Lanczos algo

Each iteration of a Krylov method requires one `\(\bm{v} \to \bm{A} \bm{v}\)`.

---

# Taste of Krylov methods: Lanczos algo

At the `\(k\)`-th iteration, _Lanczos algorithm_ outputs 
`$$\bm{q}_1, \ldots \bm{q}_{k+1} \ \text{ and } \ \nu_1 \geq \ldots \geq \nu_{k+1},$$`
extremes of which resembles extreme eigenvec/vals of `\(\bm{A}\)`.

--

&lt;img src="figure/lanczos_rits_values_vs_num_iteration.png" alt="Ritz values of Lanczos iteration" class="center" style="width: 80%;"/&gt;

---

# Taste of Krylov methods: Lanczos algo

To deploy it, just need to supply a recipe for `\(\bm{v} \to \bm{A} \bm{v}\)`:


```python
from scipy.sparse.linalg import eigsh

def multiply_by_A(v):
  ...
  return Av

A = LinearOperator((n_row, n_col), matvec=multiply_by_A)
eigenval, eigenvec = eigsh(A, k=10, which='BE')
  # Find eigenvec/val from "both ends": 5 largests &amp; 5 smallests
  # Note: the number of required iterations is &gt; k
```

**Note:**&lt;br&gt; `\(\mathcal{K}_{k + 1}(\bm{A}, \bm{v}) = \textrm{span}\{\bm{v}, \bm{A} \bm{v}, \ldots, \bm{A}^k \bm{v} \}\)` does **not** depend as much on `\(\bm{v}\)` as on `\(\bm{A}\)`. Think `\(\ \mathcal{K}_{k + 1}(\bm{A}, \bm{v}) \approx \mathcal{K}_{k + 1}(\bm{A})\)`.

---

# Taste of Krylov methods: Lanczos algo

When might you apply Lanczos algorithm?
* Find the largest eigenvalue of Fisher info `\(\bm{X}^\intercal \bm{W} \bm{X}\)` for large-scale Bayesian sparse generalized linear model&lt;br&gt; .right[(Nishimura, Schuemie, and Suchard, 2021+)]

* Find the smallest eigenvalue of `\(\bm{\Sigma}^{-1}\)` from multivariate phylogenetic probit model `\(\quad \qquad\)` (Zhang et al., 2021+)

--

* Fast PCA from sparse `\(\bm{\Sigma}^{-1}\)` graphical model?

---

# Other Krylov methods

Symmetric cases `\(\bm{A} = \bm{A}^\intercal\)`:
* _Lancozs iteration_ finds extreme eigenvector/values.
* _Conjugate gradient method_ solves `\(\bm{A} \bm{x} = \bm{b}\)`.

--

Non-symmetric cases:
* _Arnoldi iteration_ finds extreme eigenvector/values.
* _Generalized minimal residual method_ solves `\(\bm{A} \bm{x} = \bm{b}\)`.
* _LSQR_ finds minimizer of `\(\| \bm{A} \bm{x} - \bm{b} \|^2\)` (or `\(\| \bm{X} \bm{\beta} - \bm{y} \|^2\)`).

--

**Note:** 
* You often gotta use them "right" for good performance.
* Smallest eigenvec/vals are trikier under finite prec.

---

# Other Krylov methods

&lt;img src="figure/blood_anticoagulant_propensity_score_model_cg_error_plot.png" alt="Ritz values of Lanczos iteration" class="center" style="width: 70%;"/&gt;

&lt;p style="margin-top: -1ex;"&gt; &lt;/p&gt;
CG speeding up Bayesian sparse logistic regression on blood anti-coagulant data with `\(n = 72{,}489\)` &amp; `\(p = 22{,}175\)`.&lt;br&gt; .right[(Nishimura and Suchard, 2018)]

---
class: inverse, middle, center

# Summary &amp; References 

## Brief tour of iterative methods

---

# Summary
* Iterative method _do not_ require explicit storage of `\(\bm{A}\)`. Only a recipe for `\(\bm{v} \to \bm{A} \bm{v}\)` is needed.
--

* Design algorithm/software with relevant data structures in mind. e.g. When appropriate,
  - support sparse design matrix.
  - accept `function(v){...}` as input.
--

* Choose a representation of data with algorithm performance in mind.

---

# References

&lt;img src="figure/golub_and_van_loan_matrix_computation_book.png" alt="Matrix Computations by Golub and Van Loan" class="center" style="width: 55%;"/&gt; 
&lt;!-- Can't specify relative height --- I guess that the parent's height is undefined? --&gt;

---

# References

![](figure/cg_accelerated_gibbs_on_arxiv.png)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"slideNumberFormat": "%current%",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
